{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Fhaa3pEvPjh",
        "outputId": "1e080dae-c41e-4bf0-e64b-e6c7f8a259ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting underthesea\n",
            "  Downloading underthesea-6.8.0-py3-none-any.whl (20.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.9/20.9 MB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click>=6.0 in /usr/local/lib/python3.10/dist-packages (from underthesea) (8.1.7)\n",
            "Collecting python-crfsuite>=0.9.6 (from underthesea)\n",
            "  Downloading python_crfsuite-0.9.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m67.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from underthesea) (3.8.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from underthesea) (4.66.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from underthesea) (2.31.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from underthesea) (1.4.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from underthesea) (1.2.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from underthesea) (6.0.1)\n",
            "Collecting underthesea-core==1.0.4 (from underthesea)\n",
            "  Downloading underthesea_core-1.0.4-cp310-cp310-manylinux2010_x86_64.whl (657 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m657.8/657.8 kB\u001b[0m \u001b[31m54.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->underthesea) (2023.12.25)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->underthesea) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->underthesea) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->underthesea) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->underthesea) (2024.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->underthesea) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->underthesea) (1.11.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->underthesea) (3.5.0)\n",
            "Installing collected packages: underthesea-core, python-crfsuite, underthesea\n",
            "Successfully installed python-crfsuite-0.9.10 underthesea-6.8.0 underthesea-core-1.0.4\n"
          ]
        }
      ],
      "source": [
        "!pip install underthesea\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yep3-M_4vTmx"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "from underthesea import word_tokenize\n",
        "\n",
        "from keras import Input, Model\n",
        "from keras.layers import Embedding, LSTM, Dense\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "import math\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "G-yQtF_MvWcA",
        "outputId": "37bd54ef-3802-445c-d0ce-eaed2f7e4dd2"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 9998,\n  \"fields\": [\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 9996,\n        \"samples\": [\n          \"Thay cho bia m\\u1ed9, nh\\u1eefng d\\u1ea5u \\u1ea5n n\\u00e0o \\u0111\\u01b0\\u1ee3c s\\u1eed d\\u1ee5ng t\\u1ea1i C\\u00f4ng vi\\u00ean T\\u01b0\\u1edfng ni\\u1ec7m Drake?\",\n          \"Nh\\u1eefng l\\u00fd do cho s\\u1ef1 b\\u00f9ng n\\u1ed5 d\\u00e2n s\\u1ed1 trong th\\u1ebf k\\u1ef7 18 l\\u00e0 g\\u00ec?\",\n          \"Nh\\u1eefng g\\u00ec l\\u1ec5 h\\u1ed9i gi\\u1eefa tu\\u1ea7n l\\u1ec5 ngo\\u1ea1i gi\\u00e1o \\u0111\\u00e3 l\\u00e0m cho nh\\u00e0 th\\u1edd d\\u1ec5 d\\u00e0ng l\\u1eadt \\u0111\\u1ed5 h\\u01a1n l\\u00e0 lo\\u1ea1i b\\u1ecf?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answers\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8829,\n        \"samples\": [\n          \"Jo\\u00e3o Bernardo \\\"Nino\\\" Vieira\",\n          \"m\\u1ed9t \\u0111\\u1ea3ng hai (\\u0111\\u00f4i khi \\u0111\\u01b0\\u1ee3c g\\u1ecdi l\\u00e0 h\\u1ec7 th\\u1ed1ng \\\"hai \\u0111\\u1ea3ng r\\u01b0\\u1ee1i\\\")\",\n          \"1520 \\u0110\\u1ea1i l\\u1ed9 Sedgwick\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-a4c2d644-ec6a-407b-bb54-9154d500503b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>answers</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Làm thế nào mà thí nghiệm thành công?</td>\n",
              "      <td>chúng vẫn ấm và khô qua mùa đông</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Khi nào phiên tiếp theo được công bố?</td>\n",
              "      <td>12 tháng 11 năm 1962</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Ai nói rằng Chopin đặt ra \"vào thế giới rộng l...</td>\n",
              "      <td>Zdzisław Jachimecki</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Ai là quan chức cấp cao nhất trong Chính phủ N...</td>\n",
              "      <td>Thống đốc Chiết Giang</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Oklahoma có bao nhiêu trường đại học công lập?</td>\n",
              "      <td>mười một</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9993</th>\n",
              "      <td>Một nghiên cứu của ProPublica cho thấy một số ...</td>\n",
              "      <td>hơn 500.000 đô la</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9994</th>\n",
              "      <td>Tên của con tàu đầu tiên sử dụng đèn của Ediso...</td>\n",
              "      <td>Columbia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9995</th>\n",
              "      <td>Ở những nước nào quặng uranium cao cấp đáng ch...</td>\n",
              "      <td>Canada</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9996</th>\n",
              "      <td>Những trang trại sử dụng thuốc trừ sâu sinh họ...</td>\n",
              "      <td>trang trại độc canh</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9997</th>\n",
              "      <td>Thẩm phán nào là Chánh án Toà án tối cao Tuvalu?</td>\n",
              "      <td>Ngài Gordon Ward</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>9998 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a4c2d644-ec6a-407b-bb54-9154d500503b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a4c2d644-ec6a-407b-bb54-9154d500503b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a4c2d644-ec6a-407b-bb54-9154d500503b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-2afa702b-6cd7-4123-84e8-90ea3b585420\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-2afa702b-6cd7-4123-84e8-90ea3b585420')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-2afa702b-6cd7-4123-84e8-90ea3b585420 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_8904dc1c-f55c-4c4e-a865-c06afa7a4f12\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_8904dc1c-f55c-4c4e-a865-c06afa7a4f12 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                               question  \\\n",
              "0                 Làm thế nào mà thí nghiệm thành công?   \n",
              "1                 Khi nào phiên tiếp theo được công bố?   \n",
              "2     Ai nói rằng Chopin đặt ra \"vào thế giới rộng l...   \n",
              "3     Ai là quan chức cấp cao nhất trong Chính phủ N...   \n",
              "4        Oklahoma có bao nhiêu trường đại học công lập?   \n",
              "...                                                 ...   \n",
              "9993  Một nghiên cứu của ProPublica cho thấy một số ...   \n",
              "9994  Tên của con tàu đầu tiên sử dụng đèn của Ediso...   \n",
              "9995  Ở những nước nào quặng uranium cao cấp đáng ch...   \n",
              "9996  Những trang trại sử dụng thuốc trừ sâu sinh họ...   \n",
              "9997   Thẩm phán nào là Chánh án Toà án tối cao Tuvalu?   \n",
              "\n",
              "                               answers  \n",
              "0     chúng vẫn ấm và khô qua mùa đông  \n",
              "1                 12 tháng 11 năm 1962  \n",
              "2                  Zdzisław Jachimecki  \n",
              "3                Thống đốc Chiết Giang  \n",
              "4                             mười một  \n",
              "...                                ...  \n",
              "9993                 hơn 500.000 đô la  \n",
              "9994                          Columbia  \n",
              "9995                            Canada  \n",
              "9996               trang trại độc canh  \n",
              "9997                  Ngài Gordon Ward  \n",
              "\n",
              "[9998 rows x 2 columns]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv('chatbot3.csv', usecols=[0,1])\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DkGEvaW6v299"
      },
      "outputs": [],
      "source": [
        "data_questions = df['question'].values\n",
        "data_answers = df['answers'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DvxgdUyHvh6i"
      },
      "outputs": [],
      "source": [
        "#XỬ lý dữ liệu\n",
        "def clean_text(sent):\n",
        "    return re.sub(r'[!“”\"#$%&\\'()*+,-./:;<=>?@[\\]^_`{|}~]', '', sent)\n",
        "\n",
        "# hàm để chuyển Word Segmentation cho tiếng Việt\n",
        "def clean_and_word_segmentation(sent):\n",
        "    # Handle None or NaN values\n",
        "    if sent is None or (isinstance(sent, float) and math.isnan(sent)):\n",
        "        return ''\n",
        "    # Convert to string\n",
        "    sent = str(sent)\n",
        "    # Check for rare data\n",
        "    if len(sent) < 3:  # Adjust this threshold based on your data characteristics\n",
        "        return ''\n",
        "    # Clean and tokenize the text\n",
        "    return word_tokenize(clean_text(sent.lower()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fo0cjsx2vmXJ"
      },
      "outputs": [],
      "source": [
        "count_words_ques = [len(clean_text(ques).split()) for ques in data_questions]\n",
        "counter_words_ques = Counter(count_words_ques)\n",
        "\n",
        "list_count_word = []\n",
        "list_count_sent = []\n",
        "for i in counter_words_ques.items():\n",
        "    #print(i)\n",
        "    list_count_word.append(i[0])\n",
        "    list_count_sent.append(i[1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VHYaP4Afv6n0",
        "outputId": "e1226cfe-9e18-4b03-9e86-c6be2daccf2a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(9998, 9998)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(count_words_ques), len(data_questions)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11NXQldUv9is",
        "outputId": "6a062fa5-5268-4de1-963f-ae2d72bd4848"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "len sorted_ques: 7182\n",
            "len sorted_ans: 7182\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(['Làm thế nào mà thí nghiệm thành công?',\n",
              "  'Khi nào phiên tiếp theo được công bố?'],\n",
              " ['chúng vẫn ấm và khô qua mùa đông', '12 tháng 11 năm 1962'])"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "sorted_ques = []\n",
        "sorted_ans = []\n",
        "for i,count in enumerate(count_words_ques):\n",
        "    if count <= 15:\n",
        "        sorted_ques.append(data_questions[i])\n",
        "        sorted_ans.append(data_answers[i])\n",
        "\n",
        "print('len sorted_ques:', len(sorted_ques))\n",
        "print('len sorted_ans:', len(sorted_ans))\n",
        "sorted_ques[:2], sorted_ans[:2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iRHdTGlzwAFM",
        "outputId": "13414d05-d3fc-477d-aea3-db72544b0118"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['làm thế nào mà thí nghiệm thành công', 'khi nào phiên tiếp theo được công bố']\n",
            "['<START> chúng vẫn ấm và khô qua mùa đông <END>', '<START> 12 tháng 11 năm 1962 <END>']\n"
          ]
        }
      ],
      "source": [
        "# Clean and word segmentation for sorted_ques and sorted_ans\n",
        "questions = [' '.join(clean_and_word_segmentation(ques)) for ques in sorted_ques]\n",
        "answers = ['<START> ' + ' '.join(clean_and_word_segmentation(answ)) + ' <END>' for answ in sorted_ans]\n",
        "\n",
        "\n",
        "print(questions[:2])\n",
        "print(answers[:2])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FLVswyvmwFb1"
      },
      "outputs": [],
      "source": [
        "# tokenize cho questions và answers\n",
        "tokenizer = Tokenizer(filters='', lower=False)\n",
        "tokenizer.fit_on_texts(questions + answers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_d2nLGPwHh-",
        "outputId": "33d5338b-95e7-4162-862b-b57eccd2933f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary size : 8829\n"
          ]
        }
      ],
      "source": [
        "VOCAB_SIZE = len(tokenizer.word_index) + 1\n",
        "print(f'Vocabulary size : {VOCAB_SIZE}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tDDeS8GFwJY-",
        "outputId": "ec596b9e-a83d-4c77-e67c-ce5cfee8aeaf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(7182, 15)\n",
            "làm thế nào mà thí nghiệm thành công\n",
            "[26, 28, 3, 147, 1007, 461, 25, 27]\n",
            "[  26   28    3  147 1007  461   25   27    0    0    0    0    0    0\n",
            "    0]\n"
          ]
        }
      ],
      "source": [
        "tokenized_questions = tokenizer.texts_to_sequences(questions)\n",
        "maxlen_questions = 15\n",
        "encoder_inp = pad_sequences(tokenized_questions,maxlen=maxlen_questions,padding='post')\n",
        "\n",
        "print(encoder_inp.shape)\n",
        "print(questions[0])\n",
        "print(tokenized_questions[0])\n",
        "print(encoder_inp[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hEW33wG5wMMd",
        "outputId": "4d0051eb-e0da-47ea-9936-637a04f25174"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(7182, 45)\n",
            "<START> chúng vẫn ấm và khô qua mùa đông <END>\n",
            "[1, 533, 594, 2099, 13, 1472, 245, 211, 226, 2]\n",
            "[   1  533  594 2099   13 1472  245  211  226    2    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0]\n"
          ]
        }
      ],
      "source": [
        "# decoder\n",
        "tokenized_answers = tokenizer.texts_to_sequences(answers)\n",
        "maxlen_answers = np.max([len(x) for x in tokenized_answers])\n",
        "decoder_inp = pad_sequences(tokenized_answers, maxlen=maxlen_answers, padding='post')\n",
        "\n",
        "print(decoder_inp.shape)\n",
        "print(answers[0])\n",
        "print(tokenized_answers[0])\n",
        "print(decoder_inp[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QX8QW2o2wOQk",
        "outputId": "a864d978-ee07-4172-a1b1-64c0d20f4ed2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(7182, 45, 8829)\n",
            "[533, 594, 2099, 13, 1472, 245, 211, 226, 2]\n",
            "[ 533  594 2099   13 1472  245  211  226    2    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0]\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [1. 0. 0. ... 0. 0. 0.]\n",
            " [1. 0. 0. ... 0. 0. 0.]\n",
            " [1. 0. 0. ... 0. 0. 0.]]\n"
          ]
        }
      ],
      "source": [
        "for i in range(len(tokenized_answers)):\n",
        "    tokenized_answers[i] = tokenized_answers[i][1:]\n",
        "\n",
        "padded_answers = pad_sequences(tokenized_answers, maxlen=maxlen_answers, padding='post')\n",
        "decoder_final_output = to_categorical(padded_answers, VOCAB_SIZE)\n",
        "\n",
        "print(decoder_final_output.shape)\n",
        "print(tokenized_answers[0])\n",
        "print(padded_answers[0])\n",
        "print(decoder_final_output[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9hmSbo6TwQSO"
      },
      "outputs": [],
      "source": [
        "enc_inputs = Input(shape=(None,))\n",
        "enc_embedding = Embedding(VOCAB_SIZE, 200, mask_zero=True)(enc_inputs)\n",
        "_, state_h, state_c = LSTM(200, return_state=True)(enc_embedding)\n",
        "enc_states = [state_h, state_c]\n",
        "\n",
        "dec_inputs = Input(shape=(None,))\n",
        "dec_embedding = Embedding(VOCAB_SIZE, 200, mask_zero=True)(dec_inputs)\n",
        "dec_lstm = LSTM(200, return_state=True, return_sequences=True)\n",
        "\n",
        "dec_outputs, _, _ = dec_lstm(dec_embedding, initial_state=enc_states)\n",
        "dec_dense = Dense(VOCAB_SIZE, activation='softmax')\n",
        "output = dec_dense(dec_outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ve6AWo3FyMtb",
        "outputId": "99ad1691-6f0d-403c-ba5c-74eab5b535ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)        [(None, None)]               0         []                            \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)        [(None, None)]               0         []                            \n",
            "                                                                                                  \n",
            " embedding (Embedding)       (None, None, 200)            1765800   ['input_1[0][0]']             \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)     (None, None, 200)            1765800   ['input_2[0][0]']             \n",
            "                                                                                                  \n",
            " lstm (LSTM)                 [(None, 200),                320800    ['embedding[0][0]']           \n",
            "                              (None, 200),                                                        \n",
            "                              (None, 200)]                                                        \n",
            "                                                                                                  \n",
            " lstm_1 (LSTM)               [(None, None, 200),          320800    ['embedding_1[0][0]',         \n",
            "                              (None, 200),                           'lstm[0][1]',                \n",
            "                              (None, 200)]                           'lstm[0][2]']                \n",
            "                                                                                                  \n",
            " dense (Dense)               (None, None, 8829)           1774629   ['lstm_1[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 5947829 (22.69 MB)\n",
            "Trainable params: 5947829 (22.69 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = Model([enc_inputs, dec_inputs], output)\n",
        "model.compile(optimizer=RMSprop(), loss='categorical_crossentropy')\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jd6HcKh1yRNK",
        "outputId": "93f57a2d-3214-4e59-93e2-dafcbc0a652e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "224/224 [==============================] - 16s 72ms/step - loss: 2.0273\n",
            "Epoch 2/200\n",
            "224/224 [==============================] - 17s 76ms/step - loss: 2.0009\n",
            "Epoch 3/200\n",
            "224/224 [==============================] - 16s 72ms/step - loss: 1.9733\n",
            "Epoch 4/200\n",
            "224/224 [==============================] - 17s 74ms/step - loss: 1.9446\n",
            "Epoch 5/200\n",
            "224/224 [==============================] - 16s 72ms/step - loss: 1.9205\n",
            "Epoch 6/200\n",
            "224/224 [==============================] - 17s 74ms/step - loss: 1.8953\n",
            "Epoch 7/200\n",
            "224/224 [==============================] - 17s 74ms/step - loss: 1.8670\n",
            "Epoch 8/200\n",
            "224/224 [==============================] - 16s 72ms/step - loss: 1.8380\n",
            "Epoch 9/200\n",
            "224/224 [==============================] - 17s 75ms/step - loss: 1.8153\n",
            "Epoch 10/200\n",
            "224/224 [==============================] - 18s 78ms/step - loss: 1.7840\n",
            "Epoch 11/200\n",
            "224/224 [==============================] - 16s 73ms/step - loss: 1.7646\n",
            "Epoch 12/200\n",
            "224/224 [==============================] - 16s 71ms/step - loss: 1.7340\n",
            "Epoch 13/200\n",
            "224/224 [==============================] - 16s 72ms/step - loss: 1.7012\n",
            "Epoch 14/200\n",
            "224/224 [==============================] - 16s 72ms/step - loss: 1.6761\n",
            "Epoch 15/200\n",
            "224/224 [==============================] - 17s 75ms/step - loss: 1.6526\n",
            "Epoch 16/200\n",
            "224/224 [==============================] - 16s 71ms/step - loss: 1.6253\n",
            "Epoch 17/200\n",
            "224/224 [==============================] - 16s 72ms/step - loss: 1.6029\n",
            "Epoch 18/200\n",
            "224/224 [==============================] - 16s 71ms/step - loss: 1.5819\n",
            "Epoch 19/200\n",
            "224/224 [==============================] - 17s 74ms/step - loss: 1.5565\n",
            "Epoch 20/200\n",
            "224/224 [==============================] - 16s 73ms/step - loss: 1.5275\n",
            "Epoch 21/200\n",
            "224/224 [==============================] - 16s 72ms/step - loss: 1.5024\n",
            "Epoch 22/200\n",
            "224/224 [==============================] - 17s 75ms/step - loss: 1.4788\n",
            "Epoch 23/200\n",
            "224/224 [==============================] - 18s 81ms/step - loss: 1.4552\n",
            "Epoch 24/200\n",
            "224/224 [==============================] - 17s 74ms/step - loss: 1.4326\n",
            "Epoch 25/200\n",
            "224/224 [==============================] - 16s 72ms/step - loss: 1.4079\n",
            "Epoch 26/200\n",
            "224/224 [==============================] - 16s 73ms/step - loss: 1.3833\n",
            "Epoch 27/200\n",
            "224/224 [==============================] - 17s 76ms/step - loss: 1.3580\n",
            "Epoch 28/200\n",
            "224/224 [==============================] - 16s 72ms/step - loss: 1.3345\n",
            "Epoch 29/200\n",
            "224/224 [==============================] - 16s 73ms/step - loss: 1.3198\n",
            "Epoch 30/200\n",
            "224/224 [==============================] - 16s 72ms/step - loss: 1.2937\n",
            "Epoch 31/200\n",
            "224/224 [==============================] - 17s 75ms/step - loss: 1.2707\n",
            "Epoch 32/200\n",
            "224/224 [==============================] - 17s 76ms/step - loss: 1.2471\n",
            "Epoch 33/200\n",
            "224/224 [==============================] - 16s 72ms/step - loss: 1.2207\n",
            "Epoch 34/200\n",
            "224/224 [==============================] - 16s 73ms/step - loss: 1.2037\n",
            "Epoch 35/200\n",
            "224/224 [==============================] - 18s 80ms/step - loss: 1.1772\n",
            "Epoch 36/200\n",
            "224/224 [==============================] - 17s 75ms/step - loss: 1.1589\n",
            "Epoch 37/200\n",
            "224/224 [==============================] - 16s 71ms/step - loss: 1.1357\n",
            "Epoch 38/200\n",
            "224/224 [==============================] - 16s 72ms/step - loss: 1.1145\n",
            "Epoch 39/200\n",
            "224/224 [==============================] - 17s 77ms/step - loss: 1.1002\n",
            "Epoch 40/200\n",
            "224/224 [==============================] - 16s 72ms/step - loss: 1.0799\n",
            "Epoch 41/200\n",
            "224/224 [==============================] - 16s 74ms/step - loss: 1.0586\n",
            "Epoch 42/200\n",
            "224/224 [==============================] - 17s 74ms/step - loss: 1.0347\n",
            "Epoch 43/200\n",
            "224/224 [==============================] - 17s 76ms/step - loss: 1.0208\n",
            "Epoch 44/200\n",
            "224/224 [==============================] - 16s 74ms/step - loss: 0.9951\n",
            "Epoch 45/200\n",
            "224/224 [==============================] - 16s 72ms/step - loss: 0.9780\n",
            "Epoch 46/200\n",
            "224/224 [==============================] - 16s 71ms/step - loss: 0.9615\n",
            "Epoch 47/200\n",
            "224/224 [==============================] - 17s 74ms/step - loss: 0.9405\n",
            "Epoch 48/200\n",
            "224/224 [==============================] - 17s 77ms/step - loss: 0.9308\n",
            "Epoch 49/200\n",
            "224/224 [==============================] - 16s 71ms/step - loss: 0.9099\n",
            "Epoch 50/200\n",
            "224/224 [==============================] - 16s 72ms/step - loss: 0.8917\n",
            "Epoch 51/200\n",
            "224/224 [==============================] - 16s 73ms/step - loss: 0.8768\n",
            "Epoch 52/200\n",
            "224/224 [==============================] - 17s 74ms/step - loss: 0.8569\n",
            "Epoch 53/200\n",
            "224/224 [==============================] - 16s 72ms/step - loss: 0.8444\n",
            "Epoch 54/200\n",
            "224/224 [==============================] - 16s 72ms/step - loss: 0.8220\n",
            "Epoch 55/200\n",
            "224/224 [==============================] - 16s 72ms/step - loss: 0.8058\n",
            "Epoch 56/200\n",
            "224/224 [==============================] - 17s 76ms/step - loss: 0.7876\n",
            "Epoch 57/200\n",
            "224/224 [==============================] - 16s 72ms/step - loss: 0.7702\n",
            "Epoch 58/200\n",
            "224/224 [==============================] - 16s 73ms/step - loss: 0.7544\n",
            "Epoch 59/200\n",
            "224/224 [==============================] - 16s 72ms/step - loss: 0.7405\n",
            "Epoch 60/200\n",
            "224/224 [==============================] - 17s 76ms/step - loss: 0.7265\n",
            "Epoch 61/200\n",
            "224/224 [==============================] - 17s 75ms/step - loss: 0.7057\n",
            "Epoch 62/200\n",
            "224/224 [==============================] - 16s 72ms/step - loss: 0.6910\n",
            "Epoch 63/200\n",
            "224/224 [==============================] - 16s 73ms/step - loss: 0.6798\n",
            "Epoch 64/200\n",
            "224/224 [==============================] - 17s 77ms/step - loss: 0.6657\n",
            "Epoch 65/200\n",
            "224/224 [==============================] - 16s 72ms/step - loss: 0.6472\n",
            "Epoch 66/200\n",
            "224/224 [==============================] - 17s 75ms/step - loss: 0.6380\n",
            "Epoch 67/200\n",
            "224/224 [==============================] - 17s 76ms/step - loss: 0.6216\n",
            "Epoch 68/200\n",
            "224/224 [==============================] - 18s 81ms/step - loss: 0.6115\n",
            "Epoch 69/200\n",
            "224/224 [==============================] - 17s 76ms/step - loss: 0.6007\n",
            "Epoch 70/200\n",
            "224/224 [==============================] - 16s 74ms/step - loss: 0.5798\n",
            "Epoch 71/200\n",
            "224/224 [==============================] - 16s 73ms/step - loss: 0.5697\n",
            "Epoch 72/200\n",
            "224/224 [==============================] - 17s 77ms/step - loss: 0.5563\n",
            "Epoch 73/200\n",
            "224/224 [==============================] - 17s 77ms/step - loss: 0.5491\n",
            "Epoch 74/200\n",
            "224/224 [==============================] - 16s 73ms/step - loss: 0.5288\n",
            "Epoch 75/200\n",
            "224/224 [==============================] - 16s 73ms/step - loss: 0.5271\n",
            "Epoch 76/200\n",
            "224/224 [==============================] - 17s 76ms/step - loss: 0.5153\n",
            "Epoch 77/200\n",
            "224/224 [==============================] - 16s 72ms/step - loss: 0.5031\n",
            "Epoch 78/200\n",
            "224/224 [==============================] - 16s 72ms/step - loss: 0.4864\n",
            "Epoch 79/200\n",
            "224/224 [==============================] - 16s 73ms/step - loss: 0.4772\n",
            "Epoch 80/200\n",
            "224/224 [==============================] - 17s 76ms/step - loss: 0.4676\n",
            "Epoch 81/200\n",
            "224/224 [==============================] - 16s 73ms/step - loss: 0.4560\n",
            "Epoch 82/200\n",
            "224/224 [==============================] - 16s 72ms/step - loss: 0.4432\n",
            "Epoch 83/200\n",
            "224/224 [==============================] - 16s 72ms/step - loss: 0.4397\n",
            "Epoch 84/200\n",
            "224/224 [==============================] - 17s 76ms/step - loss: 0.4314\n",
            "Epoch 85/200\n",
            "224/224 [==============================] - 16s 73ms/step - loss: 0.4190\n",
            "Epoch 86/200\n",
            "224/224 [==============================] - 17s 76ms/step - loss: 0.4154\n",
            "Epoch 87/200\n",
            "224/224 [==============================] - 16s 72ms/step - loss: 0.4069\n",
            "Epoch 88/200\n",
            "224/224 [==============================] - 17s 76ms/step - loss: 0.3926\n",
            "Epoch 89/200\n",
            "224/224 [==============================] - 17s 76ms/step - loss: 0.3880\n",
            "Epoch 90/200\n",
            "224/224 [==============================] - 17s 77ms/step - loss: 0.3732\n",
            "Epoch 91/200\n",
            "224/224 [==============================] - 17s 76ms/step - loss: 0.3747\n",
            "Epoch 92/200\n",
            "224/224 [==============================] - 17s 77ms/step - loss: 0.3671\n",
            "Epoch 93/200\n",
            "224/224 [==============================] - 16s 73ms/step - loss: 0.3573\n",
            "Epoch 94/200\n",
            "224/224 [==============================] - 16s 72ms/step - loss: 0.3448\n",
            "Epoch 95/200\n",
            "224/224 [==============================] - 17s 75ms/step - loss: 0.3420\n",
            "Epoch 96/200\n",
            "224/224 [==============================] - 17s 76ms/step - loss: 0.3342\n",
            "Epoch 97/200\n",
            "224/224 [==============================] - 16s 72ms/step - loss: 0.3256\n",
            "Epoch 98/200\n",
            "224/224 [==============================] - 16s 72ms/step - loss: 0.3261\n",
            "Epoch 99/200\n",
            "224/224 [==============================] - 17s 78ms/step - loss: 0.3185\n",
            "Epoch 100/200\n",
            "224/224 [==============================] - 17s 75ms/step - loss: 0.3065\n",
            "Epoch 101/200\n",
            "224/224 [==============================] - 16s 73ms/step - loss: 0.3008\n",
            "Epoch 102/200\n",
            "224/224 [==============================] - 16s 72ms/step - loss: 0.2953\n",
            "Epoch 103/200\n",
            "224/224 [==============================] - 17s 77ms/step - loss: 0.2930\n",
            "Epoch 104/200\n",
            "224/224 [==============================] - 17s 75ms/step - loss: 0.2858\n",
            "Epoch 105/200\n",
            "224/224 [==============================] - 16s 72ms/step - loss: 0.2778\n",
            "Epoch 106/200\n",
            "224/224 [==============================] - 16s 73ms/step - loss: 0.2705\n",
            "Epoch 107/200\n",
            "224/224 [==============================] - 17s 74ms/step - loss: 0.2664\n",
            "Epoch 108/200\n",
            "224/224 [==============================] - 17s 77ms/step - loss: 0.2674\n",
            "Epoch 109/200\n",
            "224/224 [==============================] - 16s 73ms/step - loss: 0.2604\n",
            "Epoch 110/200\n",
            "224/224 [==============================] - 16s 73ms/step - loss: 0.2545\n",
            "Epoch 111/200\n",
            "224/224 [==============================] - 17s 77ms/step - loss: 0.2514\n",
            "Epoch 112/200\n",
            "224/224 [==============================] - 18s 80ms/step - loss: 0.2440\n",
            "Epoch 113/200\n",
            "224/224 [==============================] - 17s 75ms/step - loss: 0.2433\n",
            "Epoch 114/200\n",
            "224/224 [==============================] - 16s 72ms/step - loss: 0.2331\n",
            "Epoch 115/200\n",
            "224/224 [==============================] - 16s 73ms/step - loss: 0.2373\n",
            "Epoch 116/200\n",
            "224/224 [==============================] - 17s 76ms/step - loss: 0.2329\n",
            "Epoch 117/200\n",
            "224/224 [==============================] - 16s 73ms/step - loss: 0.2209\n",
            "Epoch 118/200\n",
            "224/224 [==============================] - 16s 73ms/step - loss: 0.2120\n",
            "Epoch 119/200\n",
            "224/224 [==============================] - 17s 76ms/step - loss: 0.2129\n",
            "Epoch 120/200\n",
            "224/224 [==============================] - 17s 76ms/step - loss: 0.2074\n",
            "Epoch 121/200\n",
            "224/224 [==============================] - 17s 75ms/step - loss: 0.2012\n",
            "Epoch 122/200\n",
            "224/224 [==============================] - 17s 75ms/step - loss: 0.1975\n",
            "Epoch 123/200\n",
            "224/224 [==============================] - 17s 76ms/step - loss: 0.1938\n",
            "Epoch 124/200\n",
            "224/224 [==============================] - 17s 78ms/step - loss: 0.1946\n",
            "Epoch 125/200\n",
            "224/224 [==============================] - 17s 74ms/step - loss: 0.1912\n",
            "Epoch 126/200\n",
            "224/224 [==============================] - 17s 74ms/step - loss: 0.1834\n",
            "Epoch 127/200\n",
            "224/224 [==============================] - 18s 78ms/step - loss: 0.1810\n",
            "Epoch 128/200\n",
            "224/224 [==============================] - 16s 72ms/step - loss: 0.1791\n",
            "Epoch 129/200\n",
            "224/224 [==============================] - 16s 73ms/step - loss: 0.1738\n",
            "Epoch 130/200\n",
            "224/224 [==============================] - 16s 73ms/step - loss: 0.1684\n",
            "Epoch 131/200\n",
            "224/224 [==============================] - 17s 75ms/step - loss: 0.1683\n",
            "Epoch 132/200\n",
            "224/224 [==============================] - 16s 72ms/step - loss: 0.1600\n",
            "Epoch 133/200\n",
            "224/224 [==============================] - 16s 72ms/step - loss: 0.1602\n",
            "Epoch 134/200\n",
            "224/224 [==============================] - 17s 75ms/step - loss: 0.1549\n",
            "Epoch 135/200\n",
            "224/224 [==============================] - 17s 76ms/step - loss: 0.1552\n",
            "Epoch 136/200\n",
            "224/224 [==============================] - 16s 74ms/step - loss: 0.1486\n",
            "Epoch 137/200\n",
            "224/224 [==============================] - 17s 78ms/step - loss: 0.1491\n",
            "Epoch 138/200\n",
            "224/224 [==============================] - 17s 77ms/step - loss: 0.1421\n",
            "Epoch 139/200\n",
            "224/224 [==============================] - 17s 76ms/step - loss: 0.1381\n",
            "Epoch 140/200\n",
            "224/224 [==============================] - 17s 75ms/step - loss: 0.1392\n",
            "Epoch 141/200\n",
            "224/224 [==============================] - 17s 75ms/step - loss: 0.1323\n",
            "Epoch 142/200\n",
            "224/224 [==============================] - 17s 74ms/step - loss: 0.1303\n",
            "Epoch 143/200\n",
            "224/224 [==============================] - 16s 73ms/step - loss: 0.1244\n",
            "Epoch 144/200\n",
            "224/224 [==============================] - 17s 74ms/step - loss: 0.1236\n",
            "Epoch 145/200\n",
            "224/224 [==============================] - 18s 79ms/step - loss: 0.1213\n",
            "Epoch 146/200\n",
            "224/224 [==============================] - 17s 75ms/step - loss: 0.1203\n",
            "Epoch 147/200\n",
            "224/224 [==============================] - 17s 74ms/step - loss: 0.1161\n",
            "Epoch 148/200\n",
            "224/224 [==============================] - 16s 73ms/step - loss: 0.1081\n",
            "Epoch 149/200\n",
            "224/224 [==============================] - 17s 78ms/step - loss: 0.1076\n",
            "Epoch 150/200\n",
            "224/224 [==============================] - 17s 77ms/step - loss: 0.1084\n",
            "Epoch 151/200\n",
            "224/224 [==============================] - 16s 73ms/step - loss: 0.1027\n",
            "Epoch 152/200\n",
            "224/224 [==============================] - 17s 74ms/step - loss: 0.1031\n",
            "Epoch 153/200\n",
            "224/224 [==============================] - 17s 76ms/step - loss: 0.0984\n",
            "Epoch 154/200\n",
            "224/224 [==============================] - 16s 72ms/step - loss: 0.0927\n",
            "Epoch 155/200\n",
            "224/224 [==============================] - 16s 73ms/step - loss: 0.0921\n",
            "Epoch 156/200\n",
            "224/224 [==============================] - 17s 75ms/step - loss: 0.0910\n",
            "Epoch 157/200\n",
            "224/224 [==============================] - 17s 76ms/step - loss: 0.0888\n",
            "Epoch 158/200\n",
            "224/224 [==============================] - 17s 76ms/step - loss: 0.0920\n",
            "Epoch 159/200\n",
            "224/224 [==============================] - 17s 75ms/step - loss: 0.0830\n",
            "Epoch 160/200\n",
            "224/224 [==============================] - 17s 77ms/step - loss: 0.0822\n",
            "Epoch 161/200\n",
            "224/224 [==============================] - 17s 75ms/step - loss: 0.0845\n",
            "Epoch 162/200\n",
            "224/224 [==============================] - 17s 74ms/step - loss: 0.0774\n",
            "Epoch 163/200\n",
            "224/224 [==============================] - 18s 78ms/step - loss: 0.0800\n",
            "Epoch 164/200\n",
            "224/224 [==============================] - 17s 77ms/step - loss: 0.0773\n",
            "Epoch 165/200\n",
            "224/224 [==============================] - 17s 75ms/step - loss: 0.0728\n",
            "Epoch 166/200\n",
            "224/224 [==============================] - 17s 75ms/step - loss: 0.0717\n",
            "Epoch 167/200\n",
            "224/224 [==============================] - 17s 77ms/step - loss: 0.0665\n",
            "Epoch 168/200\n",
            "224/224 [==============================] - 17s 74ms/step - loss: 0.0667\n",
            "Epoch 169/200\n",
            "224/224 [==============================] - 17s 74ms/step - loss: 0.0677\n",
            "Epoch 170/200\n",
            "224/224 [==============================] - 17s 75ms/step - loss: 0.0636\n",
            "Epoch 171/200\n",
            "224/224 [==============================] - 18s 78ms/step - loss: 0.0613\n",
            "Epoch 172/200\n",
            "224/224 [==============================] - 17s 75ms/step - loss: 0.0615\n",
            "Epoch 173/200\n",
            "224/224 [==============================] - 17s 76ms/step - loss: 0.0580\n",
            "Epoch 174/200\n",
            "224/224 [==============================] - 17s 77ms/step - loss: 0.0567\n",
            "Epoch 175/200\n",
            "224/224 [==============================] - 17s 77ms/step - loss: 0.0527\n",
            "Epoch 176/200\n",
            "224/224 [==============================] - 18s 80ms/step - loss: 0.0503\n",
            "Epoch 177/200\n",
            "224/224 [==============================] - 17s 76ms/step - loss: 0.0527\n",
            "Epoch 178/200\n",
            "224/224 [==============================] - 18s 79ms/step - loss: 0.0511\n",
            "Epoch 179/200\n",
            "224/224 [==============================] - 16s 72ms/step - loss: 0.0493\n",
            "Epoch 180/200\n",
            "224/224 [==============================] - 16s 73ms/step - loss: 0.0469\n",
            "Epoch 181/200\n",
            "224/224 [==============================] - 16s 73ms/step - loss: 0.0443\n",
            "Epoch 182/200\n",
            "224/224 [==============================] - 17s 78ms/step - loss: 0.0459\n",
            "Epoch 183/200\n",
            "224/224 [==============================] - 16s 73ms/step - loss: 0.0445\n",
            "Epoch 184/200\n",
            "224/224 [==============================] - 17s 76ms/step - loss: 0.0422\n",
            "Epoch 185/200\n",
            "224/224 [==============================] - 17s 75ms/step - loss: 0.0394\n",
            "Epoch 186/200\n",
            "224/224 [==============================] - 17s 78ms/step - loss: 0.0413\n",
            "Epoch 187/200\n",
            "224/224 [==============================] - 17s 76ms/step - loss: 0.0393\n",
            "Epoch 188/200\n",
            "224/224 [==============================] - 17s 74ms/step - loss: 0.0382\n",
            "Epoch 189/200\n",
            "224/224 [==============================] - 18s 80ms/step - loss: 0.0364\n",
            "Epoch 190/200\n",
            "224/224 [==============================] - 17s 74ms/step - loss: 0.0375\n",
            "Epoch 191/200\n",
            "224/224 [==============================] - 17s 76ms/step - loss: 0.0344\n",
            "Epoch 192/200\n",
            "224/224 [==============================] - 18s 79ms/step - loss: 0.0325\n",
            "Epoch 193/200\n",
            "224/224 [==============================] - 17s 77ms/step - loss: 0.0301\n",
            "Epoch 194/200\n",
            "224/224 [==============================] - 17s 75ms/step - loss: 0.0301\n",
            "Epoch 195/200\n",
            "224/224 [==============================] - 17s 74ms/step - loss: 0.0290\n",
            "Epoch 196/200\n",
            "224/224 [==============================] - 17s 77ms/step - loss: 0.0261\n",
            "Epoch 197/200\n",
            "224/224 [==============================] - 17s 77ms/step - loss: 0.0267\n",
            "Epoch 198/200\n",
            "224/224 [==============================] - 17s 76ms/step - loss: 0.0256\n",
            "Epoch 199/200\n",
            "224/224 [==============================] - 17s 74ms/step - loss: 0.0243\n",
            "Epoch 200/200\n",
            "224/224 [==============================] - 18s 79ms/step - loss: 0.0229\n"
          ]
        }
      ],
      "source": [
        "def data_generator(encoder_inp, decoder_inp, decoder_final_output, batch_size):\n",
        "    num_samples = len(encoder_inp)\n",
        "    while True:\n",
        "        for offset in range(0, num_samples, batch_size):\n",
        "            encoder_batch = encoder_inp[offset:offset+batch_size]\n",
        "            decoder_input_batch = decoder_inp[offset:offset+batch_size]\n",
        "            decoder_output_batch = decoder_final_output[offset:offset+batch_size]\n",
        "            yield [encoder_batch, decoder_input_batch], decoder_output_batch\n",
        "\n",
        "batch_size = 32  # Set an appropriate batch size\n",
        "model.fit(data_generator(encoder_inp, decoder_inp, decoder_final_output, batch_size),\n",
        "          steps_per_epoch=len(encoder_inp) // batch_size,\n",
        "          epochs=200)\n",
        "\n",
        "model.save('chatbot.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DH6M38ec617G",
        "outputId": "40102628-2c4a-42bb-cf97-2013a66b7a32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Inference decoder:\n",
            "Model: \"model_11\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)        [(None, None)]               0         []                            \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)     (None, None, 200)            1765800   ['input_2[0][0]']             \n",
            "                                                                                                  \n",
            " input_13 (InputLayer)       [(None, 200)]                0         []                            \n",
            "                                                                                                  \n",
            " input_14 (InputLayer)       [(None, 200)]                0         []                            \n",
            "                                                                                                  \n",
            " lstm_1 (LSTM)               [(None, None, 200),          320800    ['embedding_1[0][0]',         \n",
            "                              (None, 200),                           'input_13[0][0]',            \n",
            "                              (None, 200)]                           'input_14[0][0]']            \n",
            "                                                                                                  \n",
            " dense (Dense)               (None, None, 8829)           1774629   ['lstm_1[6][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 3861229 (14.73 MB)\n",
            "Trainable params: 3861229 (14.73 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n",
            "Inference encoder:\n",
            "Model: \"model_12\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, None, 200)         1765800   \n",
            "                                                                 \n",
            " lstm (LSTM)                 [(None, 200),             320800    \n",
            "                              (None, 200),                       \n",
            "                              (None, 200)]                       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2086600 (7.96 MB)\n",
            "Trainable params: 2086600 (7.96 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "def make_inference_models():\n",
        "    dec_state_input_h = Input(shape=(200,))\n",
        "    dec_state_input_c = Input(shape=(200,))\n",
        "    dec_states_inputs = [dec_state_input_h, dec_state_input_c]\n",
        "    dec_outputs, state_h, state_c = dec_lstm(dec_embedding,\n",
        "                                            initial_state=dec_states_inputs)\n",
        "    dec_states = [state_h, state_c]\n",
        "    dec_outputs = dec_dense(dec_outputs)\n",
        "    dec_model = Model(\n",
        "        inputs=[dec_inputs] + dec_states_inputs,\n",
        "        outputs = [dec_outputs] + dec_states)\n",
        "    print('Inference decoder:')\n",
        "    dec_model.summary()\n",
        "    print('Inference encoder:')\n",
        "    enc_model = Model(inputs=enc_inputs, outputs=enc_states)\n",
        "    enc_model.summary()\n",
        "    return enc_model, dec_model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "enc_model, dec_model = make_inference_models()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vsJSljGvkhP0"
      },
      "outputs": [],
      "source": [
        "def str_to_tokens(sentence):\n",
        "    cleaned_and_segmented_sentence = clean_and_word_segmentation(sentence)\n",
        "    if isinstance(cleaned_and_segmented_sentence, list):\n",
        "        cleaned_and_segmented_sentence = ' '.join(cleaned_and_segmented_sentence)\n",
        "    words = cleaned_and_segmented_sentence.lower().split()\n",
        "    tokens_list = list()\n",
        "    for current_word in words:\n",
        "        result = tokenizer.word_index.get(current_word)\n",
        "        if result is not None:\n",
        "            tokens_list.append(result)\n",
        "    return pad_sequences([tokens_list], maxlen=maxlen_questions, padding='post')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AMUkTxaU65jG"
      },
      "outputs": [],
      "source": [
        "def chatbot():\n",
        "    print('Bot: Xin chào!')\n",
        "\n",
        "    while True:\n",
        "        input_question = input('Question: ')\n",
        "\n",
        "        if input_question == 'bye':\n",
        "            print('Bot answer: bye')\n",
        "            break\n",
        "        states_values = enc_model.predict(str_to_tokens(input_question))\n",
        "        empty_target_seq = np.zeros((1,1))\n",
        "        empty_target_seq[0,0] = tokenizer.word_index['<START>']\n",
        "        stop_condition = False\n",
        "        decoded_translation = ''\n",
        "        while not stop_condition:\n",
        "            dec_outputs, h, c = dec_model.predict([empty_target_seq]+states_values)\n",
        "            sampled_word_index = np.argmax(dec_outputs[0,-1, :])\n",
        "            sampled_word = None\n",
        "            for word, index in tokenizer.word_index.items():\n",
        "                if sampled_word_index == index:\n",
        "                    if word != '<END>':\n",
        "                        decoded_translation += f'{word} '\n",
        "                    sampled_word = word\n",
        "\n",
        "            if sampled_word == '<END>' or len(decoded_translation.split()) > maxlen_answers:\n",
        "                stop_condition = True\n",
        "            empty_target_seq = np.zeros((1,1))\n",
        "            empty_target_seq[0,0] = sampled_word_index\n",
        "            states_values = [h,c]\n",
        "\n",
        "        print('Bot answer:', decoded_translation, '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9s2XCjVg6-Ze",
        "outputId": "de69a756-f9c4-4470-a55e-4fb4851fcf45"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bot: Xin chào!\n",
            "Question: làm thế nào để thí nghiệm\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "Bot answer: trên mực nước biển  \n",
            "\n",
            "Question: làm thế nào mà thí nghiệm thành công\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "Bot answer: chúng vẫn ấm và khô qua mùa đông  \n",
            "\n",
            "Question: Khi nào phiên tiếp theo được công bố?\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "Bot answer: 12 tháng 11 năm 1962  \n",
            "\n",
            "Question: Thẩm phán nào là Chánh án Toà án tối cao Tuvalu?\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "Bot answer: ngài gordon ward  \n",
            "\n",
            "Question: Oklahoma có bao nhiêu trường đại học công lập?\t\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "Bot answer: mười một  \n",
            "\n",
            "Question: bye\n",
            "Bot answer: bye\n"
          ]
        }
      ],
      "source": [
        "chatbot()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}